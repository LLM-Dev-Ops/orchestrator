# RAG Pipeline Example - Question Answering with Vector Search
# This workflow demonstrates a complete RAG (Retrieval-Augmented Generation) pipeline:
# 1. Embed the user's query
# 2. Search for relevant documents in a vector database
# 3. Generate an answer using an LLM with retrieved context

name: "rag-qa-pipeline"
version: "1.0"
description: "RAG-based question answering using embeddings and vector search"

steps:
  # Step 1: Embed the user's query
  - id: "embed_query"
    type: "embed"
    provider: "openai"
    model: "text-embedding-3-small"
    input: "{{ inputs.question }}"
    dimensions: 1536
    output:
      - "query_vector"  # The embedding vector
      - "embed_metadata"  # Metadata about the embedding

  # Step 2: Search for relevant documents
  - id: "search_docs"
    type: "vector_search"
    depends_on:
      - "embed_query"
    database: "pinecone"
    index: "knowledge-base"
    query: "{{ steps.embed_query.query_vector }}"
    top_k: 5
    include_metadata: true
    include_vectors: false
    output:
      - "search_results"
      - "search_metadata"

  # Step 3: Generate answer using retrieved context
  - id: "generate_answer"
    type: "llm"
    depends_on:
      - "search_docs"
    provider: "anthropic"
    model: "claude-3-5-sonnet-20241022"
    system: "You are a helpful assistant that answers questions based on the provided context. If the context doesn't contain relevant information, say so."
    prompt: |
      Context from knowledge base:
      {{#each steps.search_docs.search_results}}
      ---
      Document {{@index}}:
      {{this.metadata.text}}
      (Score: {{this.score}})
      {{/each}}
      ---

      Question: {{ inputs.question }}

      Answer based on the context above:
    temperature: 0.3
    max_tokens: 500
    output:
      - "answer"
      - "model_used"
      - "tokens_used"
