# LLM-Orchestrator Integration Patterns

**Companion Document to Requirements Analysis**
**Version:** 1.0
**Date:** November 14, 2025

---

## Table of Contents

1. [Integration Architecture Overview](#1-integration-architecture-overview)
2. [LLM-Forge Integration Patterns](#2-llm-forge-integration-patterns)
3. [LLM-Test-Bench Integration Patterns](#3-llm-test-bench-integration-patterns)
4. [LLM-Auto-Optimizer Integration Patterns](#4-llm-auto-optimizer-integration-patterns)
5. [LLM-Governance-Core Integration Patterns](#5-llm-governance-core-integration-patterns)
6. [Cross-Module Communication Protocols](#6-cross-module-communication-protocols)
7. [Real-World Workflow Examples](#7-real-world-workflow-examples)

---

## 1. Integration Architecture Overview

### 1.1 High-Level Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    LLM-Orchestrator Core                      │
│                                                               │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │   Workflow   │  │   Execution  │  │  Resilience  │      │
│  │  Definition  │→ │    Engine    │→ │    Layer     │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
│         ↓                 ↓                   ↓              │
│  ┌──────────────────────────────────────────────────────┐  │
│  │            Integration & Event Bus Layer             │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
         │              │              │              │
         ↓              ↓              ↓              ↓
  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐
  │   LLM    │  │   LLM    │  │   LLM    │  │   LLM    │
  │  Forge   │  │   Test   │  │   Auto   │  │Governance│
  │          │  │  Bench   │  │Optimizer │  │   Core   │
  └──────────┘  └──────────┘  └──────────┘  └──────────┘
```

### 1.2 Integration Principles

1. **Loose Coupling:** Modules communicate via events and well-defined APIs
2. **Async-First:** All inter-module communication is asynchronous
3. **Bidirectional:** Orchestrator both produces and consumes events
4. **Resilient:** Integration points include retry logic and circuit breakers
5. **Observable:** All integration points emit metrics and traces

---

## 2. LLM-Forge Integration Patterns

### 2.1 Tool Execution Integration

**Purpose:** Enable orchestrator to execute tools generated by LLM-Forge

**Pattern:** Direct Invocation with Dynamic Loading

```rust
// Forge-generated tool interface
#[async_trait]
pub trait ForgeTool: Send + Sync {
    fn name(&self) -> &str;
    fn schema(&self) -> &ToolSchema;
    async fn execute(&self, params: Value) -> Result<Value>;
}

// Orchestrator task wrapper
struct ToolExecutionTask {
    tool: Arc<dyn ForgeTool>,
    params: Value,
}

#[async_trait]
impl WorkflowTask for ToolExecutionTask {
    async fn execute(&self, ctx: &TaskContext) -> Result<TaskOutput> {
        // Validate params against schema
        self.tool.schema().validate(&self.params)?;

        // Execute with timeout and retry
        let result = resilient_execute(
            || self.tool.execute(self.params.clone()),
            RetryPolicy::exponential(3)
        ).await?;

        // Emit metrics
        ctx.metrics.record_tool_execution(
            self.tool.name(),
            result.duration,
            result.success
        );

        Ok(TaskOutput::Value(result.value))
    }
}
```

**Integration Points:**

1. **Tool Registry:**
   - Orchestrator maintains registry of available tools
   - Tools registered at startup or loaded dynamically
   - Schema validation on registration

2. **Schema Validation:**
   - Use Forge-generated JSON schemas
   - Validate inputs before execution
   - Type-safe parameter passing

3. **Tool Discovery:**
   ```rust
   pub struct ToolRegistry {
       tools: HashMap<String, Arc<dyn ForgeTool>>,
   }

   impl ToolRegistry {
       pub fn register(&mut self, tool: Arc<dyn ForgeTool>) {
           self.tools.insert(tool.name().to_string(), tool);
       }

       pub fn get(&self, name: &str) -> Option<Arc<dyn ForgeTool>> {
           self.tools.get(name).cloned()
       }

       // Load tools from Forge output directory
       pub fn load_from_forge(path: &Path) -> Result<Self> {
           // Discover and load .so/.dylib files
           // Or use code generation for static linking
       }
   }
   ```

---

### 2.2 Multi-Provider SDK Integration

**Purpose:** Route LLM calls to appropriate provider SDK

**Pattern:** Provider-Agnostic Client Wrapper

```rust
#[async_trait]
pub trait LLMProvider: Send + Sync {
    fn provider_name(&self) -> &str;
    async fn complete(&self, request: CompletionRequest) -> Result<CompletionResponse>;
    async fn stream(&self, request: CompletionRequest) -> Result<impl Stream<Item = Token>>;
}

pub struct ProviderRouter {
    providers: HashMap<String, Arc<dyn LLMProvider>>,
    default_provider: String,
}

impl ProviderRouter {
    pub fn route(&self, model: &str) -> Result<Arc<dyn LLMProvider>> {
        // Route based on model name prefix
        let provider_name = if model.starts_with("gpt-") {
            "openai"
        } else if model.starts_with("claude-") {
            "anthropic"
        } else {
            &self.default_provider
        };

        self.providers.get(provider_name)
            .cloned()
            .ok_or_else(|| anyhow!("Provider {} not found", provider_name))
    }
}

// Usage in workflow
struct LLMTask {
    model: String,
    prompt: String,
    router: Arc<ProviderRouter>,
}

#[async_trait]
impl WorkflowTask for LLMTask {
    async fn execute(&self, ctx: &TaskContext) -> Result<TaskOutput> {
        let provider = self.router.route(&self.model)?;

        let request = CompletionRequest {
            model: self.model.clone(),
            prompt: self.prompt.clone(),
            ..Default::default()
        };

        let response = provider.complete(request).await?;

        Ok(TaskOutput::Value(json!({
            "text": response.text,
            "model": self.model,
            "tokens": response.usage.total_tokens,
        })))
    }
}
```

**Benefits:**
- Single interface for all LLM providers
- Easy to add new providers
- Provider-specific rate limiting and retry logic
- Cost tracking per provider

---

### 2.3 Code Generation Integration

**Option A: Static Linking (Compile-Time)**

```toml
# Cargo.toml
[dependencies]
llm-forge-tools = { path = "../llm-forge/generated" }

# Build script
[build-dependencies]
llm-forge-codegen = { path = "../llm-forge/codegen" }
```

```rust
// build.rs
fn main() {
    // Generate Rust code from OpenAPI specs
    llm_forge_codegen::generate_tools(
        "tools_config.yaml",
        "src/generated/tools.rs"
    ).unwrap();
}

// Usage
use generated::tools::*;

let task = SearchTool::new(SearchParams {
    query: "rust async".into(),
    limit: 10,
});
```

**Option B: Dynamic Loading (Runtime)**

```rust
use libloading::{Library, Symbol};

pub struct DynamicTool {
    lib: Library,
    execute_fn: Symbol<'static, unsafe extern "C" fn(*const u8, usize) -> *const u8>,
}

impl DynamicTool {
    pub unsafe fn load(path: &Path) -> Result<Self> {
        let lib = Library::new(path)?;
        let execute_fn = lib.get(b"forge_tool_execute")?;
        Ok(Self { lib, execute_fn })
    }
}
```

**Recommendation:** Start with static linking for simplicity, add dynamic loading if needed for hot-reload

---

## 3. LLM-Test-Bench Integration Patterns

### 3.1 Evaluation Trigger Pattern

**Purpose:** Trigger evaluations on workflow completion

**Pattern:** Event-Driven Evaluation

```rust
use flume::{Sender, Receiver};

#[derive(Debug, Clone, Serialize)]
pub struct WorkflowCompletedEvent {
    pub workflow_id: String,
    pub timestamp: DateTime<Utc>,
    pub outputs: HashMap<String, Value>,
    pub metadata: WorkflowMetadata,
}

pub struct TestBenchConnector {
    event_tx: Sender<WorkflowCompletedEvent>,
    feedback_rx: Receiver<EvaluationResult>,
}

impl TestBenchConnector {
    pub async fn on_workflow_completed(&self, event: WorkflowCompletedEvent) {
        // Async send to avoid blocking workflow
        tokio::spawn({
            let tx = self.event_tx.clone();
            async move {
                tx.send_async(event).await.ok();
            }
        });
    }

    pub async fn poll_feedback(&self) -> Option<EvaluationResult> {
        self.feedback_rx.try_recv().ok()
    }
}

// Test-Bench consumer (runs in separate service/thread)
async fn evaluation_loop(rx: Receiver<WorkflowCompletedEvent>) {
    while let Ok(event) = rx.recv_async().await {
        // Extract evaluation dataset
        let test_cases = extract_test_cases(&event);

        // Run evaluations
        let results = run_benchmarks(test_cases).await;

        // Send feedback to orchestrator
        send_feedback(EvaluationResult {
            workflow_id: event.workflow_id,
            metrics: results,
        }).await;
    }
}
```

---

### 3.2 A/B Testing Integration

**Purpose:** Support workflow variant testing

**Pattern:** Traffic Splitting with Metrics Collection

```rust
pub struct ABTestConfig {
    pub experiment_id: String,
    pub variants: Vec<WorkflowVariant>,
    pub traffic_split: Vec<f32>, // [0.5, 0.5] for 50/50
}

pub struct WorkflowVariant {
    pub variant_id: String,
    pub workflow_def: WorkflowDefinition,
}

pub struct ABTestRouter {
    config: ABTestConfig,
    metrics_tx: Sender<VariantMetrics>,
}

impl ABTestRouter {
    pub fn select_variant(&self, user_id: &str) -> &WorkflowVariant {
        // Deterministic assignment based on user_id hash
        let hash = hash_user_id(user_id);
        let normalized = (hash % 100) as f32 / 100.0;

        let mut cumulative = 0.0;
        for (variant, split) in self.config.variants.iter().zip(&self.config.traffic_split) {
            cumulative += split;
            if normalized < cumulative {
                return variant;
            }
        }

        &self.config.variants[0] // Fallback
    }

    pub async fn execute_with_tracking(
        &self,
        user_id: &str,
        input: Value
    ) -> Result<Value> {
        let variant = self.select_variant(user_id);

        let start = Instant::now();
        let result = execute_workflow(&variant.workflow_def, input).await;
        let duration = start.elapsed();

        // Record metrics for this variant
        self.metrics_tx.send_async(VariantMetrics {
            experiment_id: self.config.experiment_id.clone(),
            variant_id: variant.variant_id.clone(),
            success: result.is_ok(),
            latency: duration,
            user_id: user_id.to_string(),
        }).await.ok();

        result
    }
}
```

**Test-Bench Analysis:**
```rust
// Test-Bench aggregates metrics and computes statistical significance
pub struct VariantAnalysis {
    pub variant_id: String,
    pub total_requests: u64,
    pub success_rate: f64,
    pub p50_latency: Duration,
    pub p95_latency: Duration,
    pub mean_quality_score: f64,
}

pub fn analyze_experiment(metrics: Vec<VariantMetrics>) -> HashMap<String, VariantAnalysis> {
    // Group by variant
    // Compute aggregate statistics
    // Run t-test for statistical significance
}
```

---

### 3.3 Continuous Evaluation Pattern

**Purpose:** Monitor workflow quality in production

**Pattern:** Sampling-Based Evaluation

```rust
pub struct ContinuousEvaluator {
    sample_rate: f32, // 0.1 = 10% of executions
    evaluators: Vec<Box<dyn Evaluator>>,
    metrics_tx: Sender<EvaluationMetrics>,
}

impl ContinuousEvaluator {
    pub async fn maybe_evaluate(&self, output: &WorkflowOutput) {
        // Sample based on configured rate
        if rand::random::<f32>() > self.sample_rate {
            return;
        }

        // Async evaluation (don't block)
        let evaluators = self.evaluators.clone();
        let output = output.clone();
        let metrics_tx = self.metrics_tx.clone();

        tokio::spawn(async move {
            for evaluator in evaluators {
                let score = evaluator.evaluate(&output).await;
                metrics_tx.send_async(EvaluationMetrics {
                    workflow_id: output.workflow_id.clone(),
                    evaluator_name: evaluator.name(),
                    score,
                    timestamp: Utc::now(),
                }).await.ok();
            }
        });
    }
}

// Example evaluator
#[async_trait]
pub trait Evaluator: Send + Sync {
    fn name(&self) -> &str;
    async fn evaluate(&self, output: &WorkflowOutput) -> f64;
}

pub struct SemanticSimilarityEvaluator {
    reference_outputs: Vec<String>,
}

#[async_trait]
impl Evaluator for SemanticSimilarityEvaluator {
    fn name(&self) -> &str { "semantic_similarity" }

    async fn evaluate(&self, output: &WorkflowOutput) -> f64 {
        let embedding = generate_embedding(&output.text).await;
        let similarities: Vec<f64> = self.reference_outputs
            .iter()
            .map(|ref_out| {
                let ref_emb = generate_embedding(ref_out).await;
                cosine_similarity(&embedding, &ref_emb)
            })
            .collect();

        similarities.iter().sum::<f64>() / similarities.len() as f64
    }
}
```

---

## 4. LLM-Auto-Optimizer Integration Patterns

### 4.1 Telemetry Export Pattern

**Purpose:** Export workflow metrics for optimization

**Pattern:** OpenTelemetry-Based Metrics

```rust
use opentelemetry::{metrics::*, KeyValue};

pub struct WorkflowMetricsExporter {
    meter: Meter,
    latency_histogram: Histogram<f64>,
    cost_counter: Counter<f64>,
    quality_gauge: Gauge<f64>,
}

impl WorkflowMetricsExporter {
    pub fn new() -> Self {
        let meter = global::meter("llm-orchestrator");

        Self {
            latency_histogram: meter
                .f64_histogram("workflow.latency")
                .with_description("Workflow execution latency in seconds")
                .init(),
            cost_counter: meter
                .f64_counter("workflow.cost")
                .with_description("Workflow execution cost in USD")
                .init(),
            quality_gauge: meter
                .f64_gauge("workflow.quality")
                .with_description("Workflow output quality score")
                .init(),
            meter,
        }
    }

    pub fn record_execution(&self, metrics: &WorkflowMetrics) {
        let labels = vec![
            KeyValue::new("workflow_id", metrics.workflow_id.clone()),
            KeyValue::new("variant", metrics.variant_id.clone()),
            KeyValue::new("model", metrics.primary_model.clone()),
        ];

        self.latency_histogram.record(
            metrics.latency.as_secs_f64(),
            &labels
        );

        self.cost_counter.add(metrics.total_cost, &labels);

        self.quality_gauge.record(metrics.quality_score, &labels);
    }
}
```

**Auto-Optimizer Consumption:**
```rust
// Auto-Optimizer queries metrics backend (Prometheus/Datadog)
pub async fn fetch_workflow_metrics(
    workflow_id: &str,
    time_range: TimeRange
) -> Result<TimeSeriesMetrics> {
    // Query metrics from observability backend
    let latency = query_metric("workflow.latency", workflow_id, time_range).await?;
    let cost = query_metric("workflow.cost", workflow_id, time_range).await?;
    let quality = query_metric("workflow.quality", workflow_id, time_range).await?;

    Ok(TimeSeriesMetrics { latency, cost, quality })
}

// Use metrics for optimization
pub async fn optimize_workflow(workflow_id: &str) -> Result<OptimizationRecommendation> {
    let metrics = fetch_workflow_metrics(workflow_id, last_24_hours()).await?;

    // Analyze trends
    if metrics.cost.mean() > COST_THRESHOLD && metrics.quality.mean() > QUALITY_THRESHOLD {
        // Suggest cheaper model
        return Ok(OptimizationRecommendation::SwitchModel {
            from: "gpt-4",
            to: "gpt-3.5-turbo",
            expected_savings: 0.80, // 80% cost reduction
            expected_quality_delta: -0.05, // 5% quality drop
        });
    }

    Ok(OptimizationRecommendation::NoChange)
}
```

---

### 4.2 Dynamic Configuration Pattern

**Purpose:** Apply optimizer recommendations without restart

**Pattern:** Hot-Reload Configuration

```rust
use tokio::sync::RwLock;
use notify::{Watcher, RecursiveMode};

pub struct DynamicWorkflowConfig {
    config: Arc<RwLock<WorkflowConfig>>,
    config_path: PathBuf,
}

impl DynamicWorkflowConfig {
    pub async fn new(config_path: PathBuf) -> Result<Self> {
        let config = Self::load_config(&config_path).await?;

        let instance = Self {
            config: Arc::new(RwLock::new(config)),
            config_path,
        };

        // Start file watcher
        instance.start_watcher().await;

        Ok(instance)
    }

    async fn load_config(path: &Path) -> Result<WorkflowConfig> {
        let content = tokio::fs::read_to_string(path).await?;
        Ok(serde_yaml::from_str(&content)?)
    }

    async fn start_watcher(&self) {
        let config = self.config.clone();
        let path = self.config_path.clone();

        tokio::spawn(async move {
            let (tx, rx) = flume::unbounded();
            let mut watcher = notify::recommended_watcher(move |res| {
                tx.send(res).ok();
            }).unwrap();

            watcher.watch(&path, RecursiveMode::NonRecursive).unwrap();

            while let Ok(Ok(event)) = rx.recv_async().await {
                if matches!(event.kind, notify::EventKind::Modify(_)) {
                    match Self::load_config(&path).await {
                        Ok(new_config) => {
                            *config.write().await = new_config;
                            tracing::info!("Configuration reloaded");
                        }
                        Err(e) => {
                            tracing::error!("Failed to reload config: {}", e);
                        }
                    }
                }
            }
        });
    }

    pub async fn get(&self) -> WorkflowConfig {
        self.config.read().await.clone()
    }
}

// Usage
let config = DynamicWorkflowConfig::new("workflow.yaml").await?;

loop {
    let current_config = config.get().await;
    execute_workflow_with_config(&current_config).await;
}
```

**Auto-Optimizer Config Update:**
```rust
// Auto-Optimizer writes updated config
pub async fn apply_optimization(
    config_path: &Path,
    recommendation: OptimizationRecommendation
) -> Result<()> {
    let mut config = load_config(config_path).await?;

    match recommendation {
        OptimizationRecommendation::SwitchModel { to, .. } => {
            config.primary_model = to;
        }
        OptimizationRecommendation::AdjustTemperature { new_temp } => {
            config.temperature = new_temp;
        }
        // ... other optimizations
    }

    // Atomic write
    let temp_path = config_path.with_extension("tmp");
    tokio::fs::write(&temp_path, serde_yaml::to_string(&config)?).await?;
    tokio::fs::rename(&temp_path, config_path).await?;

    Ok(())
}
```

---

### 4.3 Reinforcement Learning Integration

**Purpose:** Enable RL-based workflow optimization

**Pattern:** State-Action-Reward Logging

```rust
#[derive(Debug, Serialize)]
pub struct RLTransition {
    pub state: WorkflowState,
    pub action: WorkflowAction,
    pub reward: f64,
    pub next_state: WorkflowState,
}

#[derive(Debug, Serialize)]
pub struct WorkflowState {
    pub input_length: usize,
    pub task_complexity: f64,
    pub available_budget: f64,
    pub latency_requirement: Duration,
}

#[derive(Debug, Serialize)]
pub enum WorkflowAction {
    SelectModel(String),
    AdjustTemperature(f64),
    EnableCaching(bool),
    SetMaxTokens(usize),
}

pub struct RLLogger {
    transitions_tx: Sender<RLTransition>,
}

impl RLLogger {
    pub async fn log_transition(
        &self,
        state: WorkflowState,
        action: WorkflowAction,
        metrics: &WorkflowMetrics,
        next_state: WorkflowState,
    ) {
        // Compute reward function
        let reward = compute_reward(metrics);

        let transition = RLTransition {
            state,
            action,
            reward,
            next_state,
        };

        self.transitions_tx.send_async(transition).await.ok();
    }
}

fn compute_reward(metrics: &WorkflowMetrics) -> f64 {
    // Multi-objective reward: quality, cost, latency
    let quality_reward = metrics.quality_score;
    let cost_penalty = -metrics.total_cost * 10.0;
    let latency_penalty = -metrics.latency.as_secs_f64() * 0.1;

    quality_reward + cost_penalty + latency_penalty
}

// Auto-Optimizer trains RL agent on transitions
pub async fn train_rl_agent(transitions: Vec<RLTransition>) {
    // Use DQN, PPO, or other RL algorithm
    // Train agent to maximize cumulative reward
}
```

---

## 5. LLM-Governance-Core Integration Patterns

### 5.1 Policy Enforcement Middleware

**Purpose:** Enforce policies before workflow execution

**Pattern:** Tower Middleware Stack

```rust
use tower::{Service, ServiceBuilder, Layer};

// Policy trait
#[async_trait]
pub trait Policy: Send + Sync {
    fn name(&self) -> &str;
    async fn evaluate(&self, request: &WorkflowRequest) -> Result<PolicyDecision>;
}

pub enum PolicyDecision {
    Allow,
    Deny { reason: String },
    Modify { new_request: WorkflowRequest },
}

// Policy enforcement middleware
pub struct PolicyLayer {
    policies: Arc<Vec<Box<dyn Policy>>>,
}

impl<S> Layer<S> for PolicyLayer {
    type Service = PolicyService<S>;

    fn layer(&self, inner: S) -> Self::Service {
        PolicyService {
            inner,
            policies: self.policies.clone(),
        }
    }
}

pub struct PolicyService<S> {
    inner: S,
    policies: Arc<Vec<Box<dyn Policy>>>,
}

impl<S> Service<WorkflowRequest> for PolicyService<S>
where
    S: Service<WorkflowRequest> + Send,
    S::Future: Send,
{
    type Response = S::Response;
    type Error = S::Error;
    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;

    fn call(&mut self, mut request: WorkflowRequest) -> Self::Future {
        let policies = self.policies.clone();
        let mut inner = self.inner.clone();

        Box::pin(async move {
            // Evaluate all policies
            for policy in policies.iter() {
                match policy.evaluate(&request).await {
                    Ok(PolicyDecision::Allow) => continue,
                    Ok(PolicyDecision::Deny { reason }) => {
                        return Err(PolicyViolation { policy: policy.name(), reason }.into());
                    }
                    Ok(PolicyDecision::Modify { new_request }) => {
                        request = new_request;
                    }
                    Err(e) => {
                        tracing::error!("Policy {} evaluation failed: {}", policy.name(), e);
                        return Err(e.into());
                    }
                }
            }

            // All policies passed, execute workflow
            inner.call(request).await
        })
    }
}

// Example policies
pub struct PIIDetectionPolicy;

#[async_trait]
impl Policy for PIIDetectionPolicy {
    fn name(&self) -> &str { "pii_detection" }

    async fn evaluate(&self, request: &WorkflowRequest) -> Result<PolicyDecision> {
        if contains_pii(&request.input) {
            Ok(PolicyDecision::Deny {
                reason: "Input contains PII".to_string()
            })
        } else {
            Ok(PolicyDecision::Allow)
        }
    }
}

pub struct BudgetPolicy {
    max_cost_per_request: f64,
}

#[async_trait]
impl Policy for BudgetPolicy {
    fn name(&self) -> &str { "budget_limit" }

    async fn evaluate(&self, request: &WorkflowRequest) -> Result<PolicyDecision> {
        let estimated_cost = estimate_workflow_cost(request);

        if estimated_cost > self.max_cost_per_request {
            // Modify to use cheaper model
            let mut new_request = request.clone();
            new_request.config.primary_model = "gpt-3.5-turbo".to_string();

            Ok(PolicyDecision::Modify { new_request })
        } else {
            Ok(PolicyDecision::Allow)
        }
    }
}
```

---

### 5.2 Cost Tracking Integration

**Purpose:** Attribute costs to users/projects

**Pattern:** Cost Middleware with Attribution

```rust
pub struct CostTracker {
    db: Arc<Database>,
}

impl CostTracker {
    pub async fn record_cost(&self, cost_record: CostRecord) -> Result<()> {
        sqlx::query!(
            r#"
            INSERT INTO workflow_costs (
                workflow_id, user_id, organization_id, model,
                input_tokens, output_tokens, total_cost, timestamp
            )
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            "#,
            cost_record.workflow_id,
            cost_record.user_id,
            cost_record.organization_id,
            cost_record.model,
            cost_record.input_tokens as i64,
            cost_record.output_tokens as i64,
            cost_record.total_cost,
            cost_record.timestamp
        )
        .execute(&*self.db)
        .await?;

        Ok(())
    }

    pub async fn get_user_spend(
        &self,
        user_id: &str,
        period: TimePeriod
    ) -> Result<UserSpend> {
        let result = sqlx::query_as!(
            UserSpend,
            r#"
            SELECT
                user_id,
                SUM(total_cost) as total_cost,
                SUM(input_tokens) as total_input_tokens,
                SUM(output_tokens) as total_output_tokens,
                COUNT(*) as request_count
            FROM workflow_costs
            WHERE user_id = $1 AND timestamp >= $2
            GROUP BY user_id
            "#,
            user_id,
            period.start
        )
        .fetch_one(&*self.db)
        .await?;

        Ok(result)
    }
}

// Middleware integration
pub struct CostTrackingLayer {
    tracker: Arc<CostTracker>,
}

impl<S> Layer<S> for CostTrackingLayer {
    type Service = CostTrackingService<S>;

    fn layer(&self, inner: S) -> Self::Service {
        CostTrackingService {
            inner,
            tracker: self.tracker.clone(),
        }
    }
}

// Service records costs after execution
impl<S> Service<WorkflowRequest> for CostTrackingService<S>
where
    S: Service<WorkflowRequest, Response = WorkflowResponse>,
{
    async fn call(&mut self, request: WorkflowRequest) -> Result<WorkflowResponse> {
        let response = self.inner.call(request.clone()).await?;

        // Extract cost information
        let cost_record = CostRecord {
            workflow_id: response.workflow_id.clone(),
            user_id: request.user_id,
            organization_id: request.organization_id,
            model: response.metadata.model_used,
            input_tokens: response.metadata.input_tokens,
            output_tokens: response.metadata.output_tokens,
            total_cost: response.metadata.total_cost,
            timestamp: Utc::now(),
        };

        // Async record (don't block response)
        tokio::spawn({
            let tracker = self.tracker.clone();
            async move {
                tracker.record_cost(cost_record).await.ok();
            }
        });

        Ok(response)
    }
}
```

---

### 5.3 Audit Logging with Event Sourcing

**Purpose:** Immutable audit trail for compliance

**Pattern:** CQRS Event Store

```rust
use cqrs_es::*;

#[derive(Debug, Serialize, Deserialize)]
pub enum WorkflowEvent {
    ExecutionStarted {
        workflow_id: String,
        user_id: String,
        input_hash: String, // SHA256 of input
        timestamp: DateTime<Utc>,
    },
    TaskExecuted {
        task_id: String,
        task_type: String,
        duration: Duration,
        success: bool,
    },
    PolicyEvaluated {
        policy_name: String,
        decision: String,
        reason: Option<String>,
    },
    ExecutionCompleted {
        workflow_id: String,
        output_hash: String,
        total_cost: f64,
        timestamp: DateTime<Utc>,
    },
    ExecutionFailed {
        workflow_id: String,
        error: String,
        timestamp: DateTime<Utc>,
    },
}

pub struct WorkflowAuditLog {
    events: Vec<WorkflowEvent>,
}

impl Aggregate for WorkflowAuditLog {
    type Command = (); // Read-only aggregate
    type Event = WorkflowEvent;
    type Error = AuditError;
    type Services = ();

    fn aggregate_type() -> String {
        "workflow_audit".to_string()
    }

    fn apply(&mut self, event: Self::Event) {
        self.events.push(event);
    }
}

// Governance queries
pub async fn query_user_actions(
    event_store: &PostgresEventStore,
    user_id: &str,
    time_range: TimeRange,
) -> Result<Vec<WorkflowEvent>> {
    let events = event_store.load_events_by_user(user_id).await?;

    let filtered: Vec<WorkflowEvent> = events
        .into_iter()
        .filter(|event| {
            matches!(event.timestamp >= time_range.start && event.timestamp <= time_range.end)
        })
        .collect();

    Ok(filtered)
}

// Compliance report generation
pub async fn generate_compliance_report(
    event_store: &PostgresEventStore,
    organization_id: &str,
    period: TimePeriod,
) -> Result<ComplianceReport> {
    let events = event_store.load_events_by_organization(organization_id).await?;

    let total_executions = events.iter()
        .filter(|e| matches!(e, WorkflowEvent::ExecutionStarted { .. }))
        .count();

    let policy_violations = events.iter()
        .filter(|e| matches!(
            e,
            WorkflowEvent::PolicyEvaluated {
                decision: "deny",
                ..
            }
        ))
        .count();

    Ok(ComplianceReport {
        organization_id: organization_id.to_string(),
        period,
        total_executions,
        policy_violations,
        audit_events: events,
    })
}
```

---

## 6. Cross-Module Communication Protocols

### 6.1 Event Schema

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum OrchestratorEvent {
    WorkflowStarted {
        workflow_id: String,
        user_id: String,
        timestamp: DateTime<Utc>,
        metadata: WorkflowMetadata,
    },
    TaskCompleted {
        workflow_id: String,
        task_id: String,
        output: Value,
        duration: Duration,
        cost: f64,
    },
    WorkflowCompleted {
        workflow_id: String,
        outputs: HashMap<String, Value>,
        total_cost: f64,
        total_duration: Duration,
        quality_metrics: Option<QualityMetrics>,
    },
    WorkflowFailed {
        workflow_id: String,
        error: String,
        failed_task: Option<String>,
    },
}

// Event bus implementation
pub struct EventBus {
    subscribers: Arc<RwLock<HashMap<String, Vec<Sender<OrchestratorEvent>>>>>,
}

impl EventBus {
    pub async fn subscribe(&self, topic: &str) -> Receiver<OrchestratorEvent> {
        let (tx, rx) = flume::unbounded();
        self.subscribers.write().await
            .entry(topic.to_string())
            .or_default()
            .push(tx);
        rx
    }

    pub async fn publish(&self, topic: &str, event: OrchestratorEvent) {
        if let Some(subs) = self.subscribers.read().await.get(topic) {
            for tx in subs {
                tx.send_async(event.clone()).await.ok();
            }
        }
    }
}
```

### 6.2 RPC Interface (gRPC)

```protobuf
// orchestrator.proto
syntax = "proto3";

service Orchestrator {
    rpc ExecuteWorkflow(WorkflowRequest) returns (WorkflowResponse);
    rpc GetWorkflowStatus(StatusRequest) returns (StatusResponse);
    rpc CancelWorkflow(CancelRequest) returns (CancelResponse);
    rpc StreamWorkflowEvents(StreamRequest) returns (stream WorkflowEvent);
}

message WorkflowRequest {
    string workflow_id = 1;
    string user_id = 2;
    bytes input = 3; // JSON-encoded
    map<string, string> config = 4;
}

message WorkflowResponse {
    string workflow_id = 1;
    bytes output = 2;
    ExecutionMetadata metadata = 3;
}

message ExecutionMetadata {
    double total_cost = 1;
    uint64 total_tokens = 2;
    uint64 duration_ms = 3;
    string model_used = 4;
}
```

---

## 7. Real-World Workflow Examples

### 7.1 RAG Pipeline with Evaluation

```rust
use daggy::Dag;

pub async fn build_rag_pipeline() -> Dag<Box<dyn WorkflowTask>, ()> {
    let mut dag = Dag::new();

    // 1. Retrieval
    let retrieval = dag.add_node(Box::new(VectorSearchTask {
        query_model: "text-embedding-3".into(),
        index_name: "documents".into(),
        top_k: 5,
    }));

    // 2. Context Assembly
    let context = dag.add_node(Box::new(ContextAssemblyTask {
        template: "Context: {documents}\n\nQuestion: {query}".into(),
    }));

    // 3. Generation
    let generation = dag.add_node(Box::new(LLMTask {
        model: "gpt-4".into(),
        system_prompt: "Answer based on the context".into(),
    }));

    // 4. Verification (parallel with generation)
    let verification = dag.add_node(Box::new(FactCheckTask {
        model: "claude-3-opus".into(),
    }));

    // 5. Evaluation (triggered by Test-Bench)
    let evaluation = dag.add_node(Box::new(EvaluationTask {
        metrics: vec!["relevance", "faithfulness", "coherence"],
    }));

    // Define dependencies
    dag.add_edge(retrieval, context, ()).unwrap();
    dag.add_edge(context, generation, ()).unwrap();
    dag.add_edge(context, verification, ()).unwrap();
    dag.add_edge(generation, evaluation, ()).unwrap();
    dag.add_edge(verification, evaluation, ()).unwrap();

    dag
}

// Execution with Test-Bench integration
pub async fn execute_rag_with_eval(
    query: String,
    test_bench: &TestBenchConnector,
) -> Result<String> {
    let dag = build_rag_pipeline().await;
    let input = json!({ "query": query });

    let output = execute_workflow(dag, input).await?;

    // Trigger evaluation
    test_bench.on_workflow_completed(WorkflowCompletedEvent {
        workflow_id: output.workflow_id.clone(),
        outputs: output.task_outputs,
        ..Default::default()
    }).await;

    Ok(output.final_result)
}
```

---

### 7.2 Multi-Agent Collaboration

```rust
pub async fn build_multi_agent_workflow() -> Dag<Box<dyn WorkflowTask>, ConditionalEdge> {
    let mut dag = Dag::new();

    // Agent 1: Planner
    let planner = dag.add_node(Box::new(LLMTask {
        model: "gpt-4".into(),
        system_prompt: "You are a planning agent. Break down the task into steps.".into(),
        output_schema: Some(json!({
            "type": "object",
            "properties": {
                "steps": { "type": "array", "items": { "type": "string" } }
            }
        })),
    }));

    // Agent 2: Researcher
    let researcher = dag.add_node(Box::new(ToolExecutionTask {
        tool: "web_search".into(),
    }));

    // Agent 3: Writer
    let writer = dag.add_node(Box::new(LLMTask {
        model: "claude-3-sonnet".into(),
        system_prompt: "You are a writing agent. Create content based on research.".into(),
    }));

    // Agent 4: Critic (conditional)
    let critic = dag.add_node(Box::new(LLMTask {
        model: "gpt-4".into(),
        system_prompt: "You are a critic. Evaluate the quality of the content.".into(),
    }));

    // Conditional edge: only critique if quality threshold not met
    let conditional_edge = ConditionalEdge {
        condition: Box::new(|output: &TaskOutput| {
            output.get("quality_score").and_then(|v| v.as_f64())
                .map(|score| score < 0.8)
                .unwrap_or(false)
        }),
        on_true: vec![critic],
        on_false: vec![],
    };

    dag.add_edge(planner, researcher, ConditionalEdge::always()).unwrap();
    dag.add_edge(researcher, writer, ConditionalEdge::always()).unwrap();
    dag.add_edge(writer, critic, conditional_edge).unwrap();

    // Loop back to writer if critique suggests improvements
    dag.add_edge(critic, writer, ConditionalEdge {
        condition: Box::new(|output: &TaskOutput| {
            output.get("needs_revision").and_then(|v| v.as_bool())
                .unwrap_or(false)
        }),
        on_true: vec![writer],
        on_false: vec![],
    }).ok(); // Allow cycle for revision loop

    dag
}
```

---

### 7.3 Adaptive Model Selection

```rust
pub struct AdaptiveModelSelector {
    optimizer_client: AutoOptimizerClient,
    governance_client: GovernanceClient,
}

impl AdaptiveModelSelector {
    pub async fn select_model(
        &self,
        task: &WorkflowTask,
        context: &TaskContext,
    ) -> Result<String> {
        // Get recommendations from Auto-Optimizer
        let optimization = self.optimizer_client
            .recommend_model(ModelSelectionRequest {
                task_type: task.task_type(),
                input_length: context.input.len(),
                latency_requirement: context.latency_sla,
                quality_requirement: context.quality_sla,
            })
            .await?;

        // Check against Governance policies
        let budget_check = self.governance_client
            .check_budget(BudgetRequest {
                user_id: context.user_id.clone(),
                estimated_cost: optimization.estimated_cost,
            })
            .await?;

        if !budget_check.approved {
            // Fall back to cheaper model
            return Ok(optimization.fallback_model);
        }

        Ok(optimization.recommended_model)
    }
}

// Usage in workflow
pub async fn execute_with_adaptive_selection(
    task: Box<dyn WorkflowTask>,
    context: TaskContext,
    selector: &AdaptiveModelSelector,
) -> Result<TaskOutput> {
    let model = selector.select_model(&task, &context).await?;

    // Override task model
    let mut modified_task = task;
    modified_task.set_model(model);

    modified_task.execute(&context).await
}
```

---

## Conclusion

These integration patterns provide a comprehensive foundation for building a Rust-based LLM orchestration engine that seamlessly integrates with the LLM DevOps suite. Key takeaways:

1. **Modularity:** Each integration point is decoupled via events, APIs, or middleware
2. **Observability:** All integrations emit metrics and traces
3. **Resilience:** Built-in retry, circuit breaker, and timeout mechanisms
4. **Flexibility:** Support for both static and dynamic configuration
5. **Production-Ready:** Patterns used in real-world distributed systems

The patterns are designed to be composable - pick and choose based on your specific requirements, and extend as needed for custom use cases.

---

**Next Steps:**
1. Prototype key patterns (event bus, policy middleware, cost tracking)
2. Define formal API contracts between modules
3. Implement integration tests with mock modules
4. Document migration path from standalone components to integrated system
